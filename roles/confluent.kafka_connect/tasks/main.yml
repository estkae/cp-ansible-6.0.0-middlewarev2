---
- include_role:
    name: confluent.common
  when: not common_role_completed|bool

- name: Gather OS Facts
  setup:
    # Only gathers items in list, filters out the rest
    filter: "{{item}}"
    gather_subset:
      - '!all'
  loop:
    - ansible_os_family
    - ansible_fqdn

# Install Packages
- name: Install the Kafka Connect Packages
  yum:
    name: "{{item}}{{confluent_package_redhat_suffix}}"
    state: latest
  loop: "{{kafka_connect_packages}}"
  when:
    - ansible_os_family == "RedHat"
    - installation_method == "package"

- name: Install the Kafka Connect Packages
  apt:
    name: "{{item}}{{confluent_package_debian_suffix}}"
  loop: "{{kafka_connect_packages}}"
  when:
    - ansible_os_family == "Debian"
    - installation_method == "package"

# Configure environment
- name: Create Connect Distributed Group
  group:
    name: "{{kafka_connect_group}}"
    gid: "{{kafka_connect_group_gid}}"

- name: Create Connect Distributed User
  user:
    name: "{{kafka_connect_user}}"
    uid: "{{kafka_connect_user_uid}}"
    comment: "Connect Distributed User"
    system: true
    group: "{{kafka_connect_group}}"

- name: Create profile
  blockinfile:
    path: '/home/{{ kafka_connect_user }}/.bash_profile'
    block:
      # User specific environment and startup programs
      JAVA_HOME={{ confluent_jdk }}/{{ confluent_jdk_version }}
      JAVA=${JAVA_HOME}/bin/java
      HOME_CONFLUENT={{archive_destination_path}}/confluent-{{confluent_package_version}}
      PATH=$HOME_CONFLUENT/bin:/opt/java/bin:$PATH
      export JAVA_HOME JAVA HOME_CONFLUENT PATH
  become_user: "{{ kafka_connect_user }}"

- name: update ulimits
  blockinfile:
    path: /etc/security/limits.conf
    marker_begin : "<!-- BEGIN {{kafka_connect_user}} ANSIBLE MANAGED BLOCK -->"
    marker_end : "<!-- END {{kafka_connect_user}} ANSIBLE MANAGED BLOCK -->"
    insertafter: EOF
    block: |
      {{ kafka_connect_user }} hard core 0
      {{ kafka_connect_user }} soft nofile 65535
      {{ kafka_connect_user }} hard nofile 65535
      {{ kafka_connect_user }} soft nproc 4096
      {{ kafka_connect_user }} hard nproc 16384

# Archive File deployments need to create SystemD service units
# Copy the tarball's systemd service to the system
- name: Copy Kafka Connect Service from archive file to system
  copy:
    src: "{{binary_base_path}}/lib/systemd/system/{{kafka_connect.systemd_file|basename}}"
    remote_src: true
    dest: "{{kafka_connect.systemd_file}}"
    mode: 0644
    force: true
  when: installation_method == "archive"

- include_role:
    name: confluent.ssl
  vars:
    truststore_storepass: "{{kafka_connect_truststore_storepass}}"
    truststore_path: "{{kafka_connect_truststore_path}}"
    keystore_path: "{{kafka_connect_keystore_path}}"
    keystore_storepass: "{{kafka_connect_keystore_storepass}}"
    keystore_keypass: "{{kafka_connect_keystore_keypass}}"
    service_name: kafka_connect
    user: "{{kafka_connect_user}}"
    group: "{{kafka_connect_group}}"
    hostnames: "{{ [inventory_hostname, ansible_fqdn] | unique }}"
    ca_cert_path: "{{kafka_connect_ca_cert_path}}"
    cert_path: "{{kafka_connect_cert_path}}"
    key_path: "{{kafka_connect_key_path}}"
  when: >
    kafka_connect_ssl_enabled|bool or
    kafka_broker_listeners[kafka_connect_kafka_listener_name]['ssl_enabled'] | default(ssl_enabled) | bool or
    mds_tls_enabled | bool

- name: Configure Kerberos
  include_role:
    name: confluent.kerberos
  vars:
    kerberos_group: "{{kafka_connect_group}}"
    kerberos_user: "{{kafka_connect_user}}"
    kerberos_keytab_path: "{{kafka_connect_kerberos_keytab_path}}"
    kerberos_keytab_destination_path: "{{kafka_connect_keytab_path}}"
    kerberos_handler: "restart connect distributed"
  when: kafka_broker_listeners[kafka_connect_kafka_listener_name]['sasl_protocol'] | default(sasl_protocol) | normalize_sasl_protocol == 'GSSAPI'

- name: Copy Kafka Connect Files
  include_tasks: ../../tasks/copy_files.yml
  vars:
    copy_files: "{{kafka_connect_copy_files}}"
    user: "{{kafka_connect_user}}"
    group: "{{kafka_connect_group}}"
  when: kafka_connect_copy_files | length > 0

- name: Configure RBAC
  include_tasks: rbac.yml
  when: rbac_enabled|bool

- name: Create Connect Distributed Config directory
  file:
    path: "{{ kafka_connect.config_file | dirname }}"
    state: directory
    mode: 0750
    owner: "{{kafka_connect_user}}"
    group: "{{kafka_connect_group}}"

- name: Create Connect Distributed Config
  template:
    src: connect-distributed.properties.j2
    dest: "{{kafka_connect.config_file}}"
    mode: 0640
    owner: "{{kafka_connect_user}}"
    group: "{{kafka_connect_group}}"
  notify: restart connect distributed

- name: Managing Connectors
  include_tasks: connectors.yml

- name: Create Logs Directory
  file:
    path: "{{kafka_connect.appender_log_path}}"
    state: directory
    group: "{{kafka_connect_group}}"
    owner: "{{kafka_connect_user}}"
    mode: 0770

- name: Create log4j Directory
  file:
    path: "{{kafka_connect.log4j_file | dirname}}"
    owner: "{{kafka_connect_user}}"
    group: "{{kafka_connect_group}}"
    state: directory
    mode: 0750

- name: Create Connect Distributed log4j Config
  template:
    src: connect_distributed_log4j.properties.j2
    dest: "{{kafka_connect.log4j_file}}"
    mode: 0640
    owner: "{{kafka_connect_user}}"
    group: "{{kafka_connect_group}}"
  notify: restart connect distributed

- name: Enable Secrets Protection
  include_tasks: ../../tasks/secrets_protection.yml
  vars:
    encrypt_passwords: "{{ kafka_connect_secrets_protection_encrypt_passwords }}"
    properties: "{{ kafka_connect_secrets_protection_encrypt_properties }}"
    config_path: "{{ kafka_connect.config_file }}"
  when: kafka_connect_secrets_protection_enabled|bool

- name: Deploy JMX Exporter Config File
  copy:
    src: "kafka_connect.yml"
    dest: "{{kafka_connect_jmxexporter_config_path}}"
    mode: 0640
    owner: "{{kafka_connect_user}}"
    group: "{{kafka_connect_group}}"
  when: kafka_connect_jmxexporter_enabled|bool

- name: Create Service Override Directory
  file:
    path: "{{kafka_connect.systemd_override | dirname}}"
    owner: "{{kafka_connect_user}}"
    group: "{{kafka_connect_group}}"
    state: directory
    mode: 0640

- name: Write Service Overrides
  template:
    src: override.conf.j2
    dest: "{{ kafka_connect.systemd_override }}"
    mode: 0640
    owner: "{{kafka_connect_user}}"
    group: "{{kafka_connect_group}}"
  notify: restart connect distributed

- name: Certs were Updated - Trigger Restart
  command: /bin/true
  notify: restart connect distributed
  when: certs_updated|bool

- meta: flush_handlers

- name: Start Connect Service
  systemd:
    name: "{{kafka_connect_service_name}}"
    enabled: true
    state: started

- name: Health Check
  include_tasks: health_check.yml
  when:
    - health_checks_enabled|bool
    - not ansible_check_mode

- name: Register connector configs and remove deleted connectors
  kafka_connectors:
    connect_url: "{{kafka_connect_http_protocol}}://{{groups['kafka_connect'][0]}}:{{kafka_connect_rest_port}}/connectors"
    active_connectors: "{{ kafka_connect_connectors }}"
  when: kafka_connect_connectors is defined
  run_once: true
